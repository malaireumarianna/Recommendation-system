{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42e98f3-744a-40da-af4b-5cdd635d12f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0a0+e000cf0ad9.nv24.10)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.14.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m294.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m670.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.29.2 sentence_transformers-3.4.1 tokenizers-0.21.0 transformers-4.49.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n",
      "Downloading kagglehub-0.3.10-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sentence_transformers\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff7219-e7fa-4b05-978c-0af0eae117aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298cd59-45c4-4cd9-85cb-9a570e1b5bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3292f66-509e-43a2-bcbb-131f4666b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3c74f-bf5a-402f-92ff-db1ef18d72dd",
   "metadata": {},
   "source": [
    "1. Download and read dataset.\n",
    "   \n",
    "It contains 3 tables - 'Users.csv', 'Ratings.csv', 'Books.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d9b81-6d6b-4f5f-a8df-5bf45a172d0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdb03f0-855d-4e87-9319-359b402b71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jovyan/.cache/kagglehub/datasets/arashnic/book-recommendation-dataset/versions/3\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"arashnic/book-recommendation-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419de1df-ed5c-44da-9847-cd0f182ccadf",
   "metadata": {},
   "outputs": [],
   "source": [
    " path = '/home/jovyan/.cache/kagglehub/datasets/arashnic/book-recommendation-dataset/versions/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b086c1-09c5-458f-be47-540ac65a5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DeepRec.png', 'Users.csv', 'Ratings.csv', 'Books.csv', 'classicRec.png', 'recsys_taxonomy2.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb78934b-fbdd-423c-9669-8978f88cb979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/1806486744.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(os.path.join(path, 'Books.csv'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp;amp; Company</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0393045218.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271355</th>\n",
       "      <td>0440400988</td>\n",
       "      <td>There's a Bat in Bunk Five</td>\n",
       "      <td>Paula Danziger</td>\n",
       "      <td>1988</td>\n",
       "      <td>Random House Childrens Pub (Mm)</td>\n",
       "      <td>http://images.amazon.com/images/P/0440400988.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0440400988.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0440400988.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271356</th>\n",
       "      <td>0525447644</td>\n",
       "      <td>From One to One Hundred</td>\n",
       "      <td>Teri Sloat</td>\n",
       "      <td>1991</td>\n",
       "      <td>Dutton Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0525447644.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0525447644.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0525447644.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271357</th>\n",
       "      <td>006008667X</td>\n",
       "      <td>Lily Dale : The True Story of the Town that Ta...</td>\n",
       "      <td>Christine Wicker</td>\n",
       "      <td>2004</td>\n",
       "      <td>HarperSanFrancisco</td>\n",
       "      <td>http://images.amazon.com/images/P/006008667X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/006008667X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/006008667X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271358</th>\n",
       "      <td>0192126040</td>\n",
       "      <td>Republic (World's Classics)</td>\n",
       "      <td>Plato</td>\n",
       "      <td>1996</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0192126040.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0192126040.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0192126040.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271359</th>\n",
       "      <td>0767409752</td>\n",
       "      <td>A Guided Tour of Rene Descartes' Meditations o...</td>\n",
       "      <td>Christopher  Biffle</td>\n",
       "      <td>2000</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>http://images.amazon.com/images/P/0767409752.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0767409752.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0767409752.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271360 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                         Book-Title  \\\n",
       "0       0195153448                                Classical Mythology   \n",
       "1       0002005018                                       Clara Callan   \n",
       "2       0060973129                               Decision in Normandy   \n",
       "3       0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4       0393045218                             The Mummies of Urumchi   \n",
       "...            ...                                                ...   \n",
       "271355  0440400988                         There's a Bat in Bunk Five   \n",
       "271356  0525447644                            From One to One Hundred   \n",
       "271357  006008667X  Lily Dale : The True Story of the Town that Ta...   \n",
       "271358  0192126040                        Republic (World's Classics)   \n",
       "271359  0767409752  A Guided Tour of Rene Descartes' Meditations o...   \n",
       "\n",
       "                 Book-Author Year-Of-Publication  \\\n",
       "0         Mark P. O. Morford                2002   \n",
       "1       Richard Bruce Wright                2001   \n",
       "2               Carlo D'Este                1991   \n",
       "3           Gina Bari Kolata                1999   \n",
       "4            E. J. W. Barber                1999   \n",
       "...                      ...                 ...   \n",
       "271355        Paula Danziger                1988   \n",
       "271356            Teri Sloat                1991   \n",
       "271357      Christine Wicker                2004   \n",
       "271358                 Plato                1996   \n",
       "271359   Christopher  Biffle                2000   \n",
       "\n",
       "                                               Publisher  \\\n",
       "0                                Oxford University Press   \n",
       "1                                  HarperFlamingo Canada   \n",
       "2                                        HarperPerennial   \n",
       "3                                   Farrar Straus Giroux   \n",
       "4                             W. W. Norton &amp; Company   \n",
       "...                                                  ...   \n",
       "271355                   Random House Childrens Pub (Mm)   \n",
       "271356                                      Dutton Books   \n",
       "271357                                HarperSanFrancisco   \n",
       "271358                           Oxford University Press   \n",
       "271359  McGraw-Hill Humanities/Social Sciences/Languages   \n",
       "\n",
       "                                              Image-URL-S  \\\n",
       "0       http://images.amazon.com/images/P/0195153448.0...   \n",
       "1       http://images.amazon.com/images/P/0002005018.0...   \n",
       "2       http://images.amazon.com/images/P/0060973129.0...   \n",
       "3       http://images.amazon.com/images/P/0374157065.0...   \n",
       "4       http://images.amazon.com/images/P/0393045218.0...   \n",
       "...                                                   ...   \n",
       "271355  http://images.amazon.com/images/P/0440400988.0...   \n",
       "271356  http://images.amazon.com/images/P/0525447644.0...   \n",
       "271357  http://images.amazon.com/images/P/006008667X.0...   \n",
       "271358  http://images.amazon.com/images/P/0192126040.0...   \n",
       "271359  http://images.amazon.com/images/P/0767409752.0...   \n",
       "\n",
       "                                              Image-URL-M  \\\n",
       "0       http://images.amazon.com/images/P/0195153448.0...   \n",
       "1       http://images.amazon.com/images/P/0002005018.0...   \n",
       "2       http://images.amazon.com/images/P/0060973129.0...   \n",
       "3       http://images.amazon.com/images/P/0374157065.0...   \n",
       "4       http://images.amazon.com/images/P/0393045218.0...   \n",
       "...                                                   ...   \n",
       "271355  http://images.amazon.com/images/P/0440400988.0...   \n",
       "271356  http://images.amazon.com/images/P/0525447644.0...   \n",
       "271357  http://images.amazon.com/images/P/006008667X.0...   \n",
       "271358  http://images.amazon.com/images/P/0192126040.0...   \n",
       "271359  http://images.amazon.com/images/P/0767409752.0...   \n",
       "\n",
       "                                              Image-URL-L  \n",
       "0       http://images.amazon.com/images/P/0195153448.0...  \n",
       "1       http://images.amazon.com/images/P/0002005018.0...  \n",
       "2       http://images.amazon.com/images/P/0060973129.0...  \n",
       "3       http://images.amazon.com/images/P/0374157065.0...  \n",
       "4       http://images.amazon.com/images/P/0393045218.0...  \n",
       "...                                                   ...  \n",
       "271355  http://images.amazon.com/images/P/0440400988.0...  \n",
       "271356  http://images.amazon.com/images/P/0525447644.0...  \n",
       "271357  http://images.amazon.com/images/P/006008667X.0...  \n",
       "271358  http://images.amazon.com/images/P/0192126040.0...  \n",
       "271359  http://images.amazon.com/images/P/0767409752.0...  \n",
       "\n",
       "[271360 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "books = pd.read_csv(os.path.join(path, 'Books.csv'))\n",
    "books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be53f953-8fee-4662-ba35-7a6fc570b215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID                            Location   Age\n",
       "0        1                  nyc, new york, usa   NaN\n",
       "1        2           stockton, california, usa  18.0\n",
       "2        3     moscow, yukon territory, russia   NaN\n",
       "3        4           porto, v.n.gaia, portugal  17.0\n",
       "4        5  farnborough, hants, united kingdom   NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(os.path.join(path, 'Users.csv'))\n",
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c0fd0fa-6eba-4f5c-b08a-bc7c85ca6461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276725</td>\n",
       "      <td>034545104X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276727</td>\n",
       "      <td>0446520802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149775</th>\n",
       "      <td>276704</td>\n",
       "      <td>1563526298</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149776</th>\n",
       "      <td>276706</td>\n",
       "      <td>0679447156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149777</th>\n",
       "      <td>276709</td>\n",
       "      <td>0515107662</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149778</th>\n",
       "      <td>276721</td>\n",
       "      <td>0590442449</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149779</th>\n",
       "      <td>276723</td>\n",
       "      <td>05162443314</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1149780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User-ID         ISBN  Book-Rating\n",
       "0         276725   034545104X            0\n",
       "1         276726   0155061224            5\n",
       "2         276727   0446520802            0\n",
       "3         276729   052165615X            3\n",
       "4         276729   0521795028            6\n",
       "...          ...          ...          ...\n",
       "1149775   276704   1563526298            9\n",
       "1149776   276706   0679447156            0\n",
       "1149777   276709   0515107662           10\n",
       "1149778   276721   0590442449           10\n",
       "1149779   276723  05162443314            8\n",
       "\n",
       "[1149780 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(os.path.join(path, 'Ratings.csv'))\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f5fc2-76ed-48c5-9308-78b6e3f05c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3eb853a-d1a0-491e-a8a1-c3ce14609f23",
   "metadata": {},
   "source": [
    "This script below processes book descriptions generated in different file from book dataset by cleaning the text, removing stopwords, and lemmatizing words to create a new preprocessed version of the descriptions. The script uses NLTK (Natural Language Toolkit) for text processing and applies the transformations to a Pandas DataFrame.\n",
    "\n",
    "Steps taken for text processing:\n",
    "\n",
    "-- convert text to lowercase: This ensures uniform processing and avoids treating \"The\" and \"the\" as different words.\n",
    "\n",
    "-- remove special characters and numbers: Removes everything except letters and spaces (i.e., numbers, punctuation, special symbols).\n",
    "\n",
    "-- tokenization: Splits text into individual words (tokens).\n",
    "\n",
    "-- lemmatizer: Used to reduce words to their root form.\n",
    "\n",
    "-- stop_words: Loads a set of common words that should be removed (e.g., \"the\", \"is\", \"and\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2707ea01-6a59-49bb-ab64-be3f1dd4e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "processed_df = pd.read_pickle('books_w_descript.pkl')\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: Lowercase, remove special chars, stop words, and lemmatize.\"\"\"\n",
    "    if not isinstance(text, str):  # Handle NaN cases\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    cleaned_text = \" \".join(lemmatizer.lemmatize(word) for word in words if word not in stop_words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "processed_df['Description_vector'] = processed_df['Description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27203e72-0640-45cc-9be5-398940f9c0c5",
   "metadata": {},
   "source": [
    "Code below applies TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to preprocessed book descriptions, transforming text into a numerical representation suitable for machine learning models. TfidfVectorizer(...) creates a TF-IDF transformation object.\n",
    "\n",
    "Parameters: max_features=5000: Limits the vocabulary to the top 5000 most frequent words (based on TF-IDF scores). Reduces dimensionality while retaining important features\n",
    "\n",
    "Also, due to inconsistency, script ensures that only books present in both the ratings dataset and book descriptions are considered. It then maps book indices correctly, and finally converts the processed TF-IDF features into a PyTorch tensor for further use in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11045139-2af2-4b39-ab2e-8039e4483bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (270146, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize preprocessed descriptions\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "\n",
    "\n",
    "# Load book dataset and filter books that are in ratings\n",
    "books_filtered = processed_df[processed_df[\"ISBN\"].isin(ratings[\"ISBN\"])].reset_index(drop=True)\n",
    "\n",
    "# Create a mapping only for books that exist in both ratings and descriptions\n",
    "book_map = {isbn: i for i, isbn in enumerate(books_filtered[\"ISBN\"].unique())}\n",
    "\n",
    "# Apply mapping to ratings (removing books that do not have descriptions)\n",
    "ratings = ratings[ratings[\"ISBN\"].isin(book_map)].copy()\n",
    "ratings[\"Book-Index\"] = ratings[\"ISBN\"].map(book_map)\n",
    "\n",
    "# Ensure that the number of books in ratings matches the number of books in book_desc_features\n",
    "num_books = len(book_map)\n",
    "\n",
    "# Generate new book_desc_features matrix that matches the book indices\n",
    "tfidf_matrix = tfidf.fit_transform(books_filtered[\"Description_vector\"])\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)  # (num_books, num_features)\n",
    "\n",
    "book_desc_features = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Assert to check sizes match\n",
    "assert book_desc_features.shape[0] == num_books, \"Mismatch in book description matrix size\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b190f23-4b69-4621-a96e-d14e0ee8ffec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c372f5-01b9-4f7a-a61f-f49524038077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings (dictionary-based encoding)\n",
    "user_map = {id: i for i, id in enumerate(ratings[\"User-ID\"].unique())}\n",
    "\n",
    "# Apply mappings\n",
    "ratings[\"User-Index\"] = ratings[\"User-ID\"].map(user_map)\n",
    "\n",
    "# Get total number of unique users and books\n",
    "num_users = len(user_map)\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5124e38-8cfa-46d9-bae7-90082319ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Index</th>\n",
       "      <th>User-Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1018939</th>\n",
       "      <td>244298</td>\n",
       "      <td>3492232949</td>\n",
       "      <td>0</td>\n",
       "      <td>269449</td>\n",
       "      <td>81387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423934</th>\n",
       "      <td>101209</td>\n",
       "      <td>0451204026</td>\n",
       "      <td>0</td>\n",
       "      <td>228637</td>\n",
       "      <td>34236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975704</th>\n",
       "      <td>235105</td>\n",
       "      <td>0425147584</td>\n",
       "      <td>0</td>\n",
       "      <td>7163</td>\n",
       "      <td>78223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956372</th>\n",
       "      <td>231210</td>\n",
       "      <td>0440237025</td>\n",
       "      <td>0</td>\n",
       "      <td>9965</td>\n",
       "      <td>76865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931919</th>\n",
       "      <td>226031</td>\n",
       "      <td>0449126625</td>\n",
       "      <td>0</td>\n",
       "      <td>206937</td>\n",
       "      <td>75188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289631</th>\n",
       "      <td>69211</td>\n",
       "      <td>0312985207</td>\n",
       "      <td>9</td>\n",
       "      <td>6041</td>\n",
       "      <td>23796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409225</th>\n",
       "      <td>98391</td>\n",
       "      <td>0515132756</td>\n",
       "      <td>8</td>\n",
       "      <td>129056</td>\n",
       "      <td>33325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148815</th>\n",
       "      <td>33580</td>\n",
       "      <td>0446522856</td>\n",
       "      <td>0</td>\n",
       "      <td>43773</td>\n",
       "      <td>11808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750911</th>\n",
       "      <td>181944</td>\n",
       "      <td>0316602051</td>\n",
       "      <td>0</td>\n",
       "      <td>2415</td>\n",
       "      <td>60742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137978</th>\n",
       "      <td>31315</td>\n",
       "      <td>0446356751</td>\n",
       "      <td>0</td>\n",
       "      <td>59599</td>\n",
       "      <td>10986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>824904 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         User-ID        ISBN  Book-Rating  Book-Index  User-Index\n",
       "1018939   244298  3492232949            0      269449       81387\n",
       "423934    101209  0451204026            0      228637       34236\n",
       "975704    235105  0425147584            0        7163       78223\n",
       "956372    231210  0440237025            0        9965       76865\n",
       "931919    226031  0449126625            0      206937       75188\n",
       "...          ...         ...          ...         ...         ...\n",
       "289631     69211  0312985207            9        6041       23796\n",
       "409225     98391  0515132756            8      129056       33325\n",
       "148815     33580  0446522856            0       43773       11808\n",
       "750911    181944  0316602051            0        2415       60742\n",
       "137978     31315  0446356751            0       59599       10986\n",
       "\n",
       "[824904 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92504325-8747-4269-89f4-8a0440ebcfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_min = train[\"Book-Rating\"].min()\n",
    "ratings_max = train[\"Book-Rating\"].max()\n",
    "\n",
    "train[\"Book-Rating\"] = (train[\"Book-Rating\"] - ratings_min) / (ratings_max - ratings_min)\n",
    "test[\"Book-Rating\"] = (test[\"Book-Rating\"] - ratings_min) / (ratings_max - ratings_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6a4ea-2941-4792-9105-7f301396c19a",
   "metadata": {},
   "source": [
    "\n",
    "This code below defines a custom PyTorch Dataset (BookDataset) for training a machine learning model on book ratings, by using both user interactions and book descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03a96b7e-7ec8-4cf6-a939-99583e1b2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "class BookDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Book Ratings including Book Descriptions\"\"\"\n",
    "    def __init__(self, df, book_desc_features):\n",
    "        self.users = torch.tensor(df[\"User-Index\"].values, dtype=torch.long)\n",
    "        self.books = torch.tensor(df[\"Book-Index\"].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df[\"Book-Rating\"].values, dtype=torch.float32)\n",
    "\n",
    "        # Ensure book indices are within bounds\n",
    "        self.books = torch.clamp(self.books, 0, book_desc_features.shape[0] - 1)\n",
    "\n",
    "        # Assign book descriptions safely\n",
    "        self.book_desc_features = book_desc_features[self.books]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.books[idx], self.book_desc_features[idx], self.ratings[idx]\n",
    "\n",
    "# Update DataLoader\n",
    "train_dataset = BookDataset(train, book_desc_features)\n",
    "test_dataset = BookDataset(test, book_desc_features)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Split train dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% training\n",
    "val_size = len(train_dataset) - train_size  # 20% validation\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create new DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146ec92-d672-4127-82bd-a11a7b753c5a",
   "metadata": {},
   "source": [
    "Below is defined a Neural Collaborative Filtering (NCF) model that integrates book descriptions as additional features. The model is designed to predict user ratings for books using user embeddings, book embeddings, and book description embeddings.\n",
    "\n",
    "\n",
    "The model converts IDs into dense 50D vectors using embeddings.\n",
    "\n",
    "Converts TF-IDF features of Book Descriptions into dense vectors.\n",
    "\n",
    "Merges User, Book, & Description Features.\n",
    "\n",
    "Passes Through Fully Connected Layers predicting a single rating value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e4b001c-30b0-4be4-9408-84b2d361693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class NCFWithDescription(nn.Module):\n",
    "    def __init__(self, num_users, num_books, desc_dim, embedding_dim=50, dropout_rate=0.5):\n",
    "        super(NCFWithDescription, self).__init__()\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n",
    "        self.desc_fc = nn.Linear(desc_dim, embedding_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim * 3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "        #self.relu = nn.ReLU() \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2) # Use LeakyReLU with negative slope 0.2\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "\n",
    "    def forward(self, user, book, book_desc):\n",
    "        user_emb = self.user_embedding(user)\n",
    "        book_emb = self.book_embedding(book)\n",
    "        desc_emb = self.leaky_relu(self.desc_fc(book_desc))\n",
    "\n",
    "        x = torch.cat([user_emb, book_emb, desc_emb], dim=1)\n",
    "\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout here\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout here\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x.squeeze()\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 50\n",
    "desc_dim = book_desc_features.shape[1]  # Number of features in description embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NCFWithDescription(num_users, num_books, desc_dim, embedding_dim, dropout_rate=0.3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cd4327c-c4dc-43d2-969a-b01f9f1894e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.1550, Val Loss: 0.1449, LR: 0.000100\n",
      "Epoch 2/30, Train Loss: 0.1462, Val Loss: 0.1427, LR: 0.000100\n",
      "Epoch 3/30, Train Loss: 0.1432, Val Loss: 0.1406, LR: 0.000100\n",
      "Epoch 4/30, Train Loss: 0.1404, Val Loss: 0.1379, LR: 0.000100\n",
      "Epoch 5/30, Train Loss: 0.1375, Val Loss: 0.1350, LR: 0.000100\n",
      "Epoch 6/30, Train Loss: 0.1340, Val Loss: 0.1316, LR: 0.000100\n",
      "Epoch 7/30, Train Loss: 0.1300, Val Loss: 0.1278, LR: 0.000100\n",
      "Epoch 8/30, Train Loss: 0.1255, Val Loss: 0.1236, LR: 0.000100\n",
      "Epoch 9/30, Train Loss: 0.1208, Val Loss: 0.1197, LR: 0.000100\n",
      "Epoch 10/30, Train Loss: 0.1167, Val Loss: 0.1168, LR: 0.000100\n",
      "Epoch 11/30, Train Loss: 0.1134, Val Loss: 0.1150, LR: 0.000100\n",
      "Epoch 12/30, Train Loss: 0.1108, Val Loss: 0.1140, LR: 0.000100\n",
      "Epoch 13/30, Train Loss: 0.1087, Val Loss: 0.1136, LR: 0.000100\n",
      "Epoch 14/30, Train Loss: 0.1065, Val Loss: 0.1136, LR: 0.000100\n",
      "Epoch 15/30, Train Loss: 0.1032, Val Loss: 0.1144, LR: 0.000100\n",
      "Epoch 16/30, Train Loss: 0.0983, Val Loss: 0.1164, LR: 0.000050\n",
      "Epoch 17/30, Train Loss: 0.0875, Val Loss: 0.1214, LR: 0.000050\n",
      "Epoch 18/30, Train Loss: 0.0830, Val Loss: 0.1237, LR: 0.000050\n",
      "Epoch 19/30, Train Loss: 0.0799, Val Loss: 0.1259, LR: 0.000025\n",
      "Epoch 20/30, Train Loss: 0.0734, Val Loss: 0.1283, LR: 0.000025\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Loss function & optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Add weight decay for L2 regularization\n",
    "\n",
    "# Learning rate scheduler: Reduce LR when validation loss stops improving\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stopping_patience = 8\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for users, books, book_descs, ratings in train_loader:\n",
    "        users, books, book_descs, ratings = users.to(device), books.to(device), book_descs.to(device), ratings.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(users, books, book_descs)\n",
    "        loss = criterion(predictions, ratings)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for users, books, book_descs, ratingss in val_loader:\n",
    "            users, books, book_descs, ratingss = users.to(device), books.to(device), book_descs.to(device), ratingss.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(users, books, book_descs)\n",
    "            loss = criterion(predictions, ratingss)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr'] \n",
    "\n",
    "    # Check if validation loss improved\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Stop if validation loss increases \n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca7f4c6d-d2dc-4188-9556-4dc8af8209e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/1495886655.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Test Loss: 0.2267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "\n",
    "\n",
    "model = NCFWithDescription(num_users, num_books, desc_dim, embedding_dim, dropout_rate=0.3).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Add weight decay for L2 regularization\n",
    "\n",
    "print(f\"Model loaded\")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()  # Set model to evaluation mode (important)\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for users, books, book_descs, ratingss in test_loader:\n",
    "        users, books, book_descs, ratingss = users.to(device), books.to(device), book_descs.to(device), ratingss.to(device)\n",
    "\n",
    "        # Forward pass (prediction)\n",
    "        predictions = model(users, books, book_descs)\n",
    "        loss = criterion(predictions, ratingss)  # Compute loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute average test loss\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20373348-0fbd-47c7-add9-f34b0c4e2ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7493e2d3-4461-440f-9583-d724178f0e4d",
   "metadata": {},
   "source": [
    "\n",
    "Function defined below is returning top 5 recommended books for specified user.\n",
    "\n",
    "Let's see what our model would recommend to read next for user '276729'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2be7766-61e2-420e-8263-b87a65887c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help!: Level 1 by Philip Prowse (ISBN: 052165615X), Description: This is the first book in the series that teaches young children the basics of English. It includes an introduction to the alphabet, the sounds of English, the letters of the alphabet, and the sounds of words. The book also includes a phonics section that teaches children to read words using the sounds of English. This book is ideal for children who are learning English as a second language.\n",
      "The Amsterdam Connection : Level 4 (Cambridge English Readers) by Sue Leather (ISBN: 0521795028), Description: In this book, students will learn about the Amsterdam connection and how it affected the world. They will learn about the Amsterdam connection and how it affected the world. They will also learn about the history of Amsterdam. They will learn about the history of Amsterdam. They will also learn about the history of Amsterdam. They will also learn about the history of Amsterdam. They will also learn about the history of Amsterdam. They will also learn about the history of Amsterdam. They will also learn about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/2992480573.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings1[\"ISBN\"] = ratings1[\"ISBN\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ratings[\"User-ID\"] = ratings[\"User-ID\"].astype(str)  # Convert User-ID to string\n",
    "ratings1 = ratings[ratings[\"User-ID\"] == \"276729\"]\n",
    "ratings1\n",
    "\n",
    "ratings1[\"ISBN\"] = ratings1[\"ISBN\"].astype(str)\n",
    "books_filtered[\"ISBN\"] = books_filtered[\"ISBN\"].astype(str)\n",
    "\n",
    "books_filtered1 = books_filtered[books_filtered[\"ISBN\"].isin(ratings1[\"ISBN\"])]\n",
    "\n",
    "books_filtered1 = books_filtered1.to_dict(orient=\"records\")  # Convert to list of dictionaries\n",
    "\n",
    "books_filtered1\n",
    "for book in books_filtered1:\n",
    "    print(f\"{book['Book-Title']} by {book['Book-Author']} (ISBN: {book['ISBN']}), Description: {book['Description']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "177b3f1b-df51-41c5-9d64-0c3a75f72708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joshua by Joseph F Girzone (ISBN: 0020198906), Predicted Rating: 0.19, Description: Joshua is a story of the life of a man named Joshua who lived in the time of the Judges. He was the son of a man named Jacob, who was a shepherd in the land of Canaan. Joshua was a good shepherd, and he was very faithful to his sheep. One day, the Lord told Joshua to go to the land of Canaan and take it for the people of Israel. Joshua was very excited and he went to the land of Canaan, but he was not allowed to go into the land\n",
      "Blind Man's Bluff: The Untold Story of American Submarine Espionage by Sherry Sontag (ISBN: 006103004X), Predicted Rating: 0.19, Description: 'In this fascinating account of a secret war, Sherry Sontag takes us on a gripping journey through the shadowy world of submarine espionage. In the 1960s and 70s, America's most elite Navy officers were engaged in a high-stakes game of cat and mouse with the Soviet Union's navy. The Soviets had a fleet of nuclear submarines, and the Americans had the technology to detect and destroy them. But the Soviets had the advantage of knowing where the Americans were, and the Americans had the advantage of\n",
      "But Not For Me (Silhouette Special Edition, No. 1472) by Annette Broadrick (ISBN: 037324472X), Predicted Rating: 0.19, Description: A woman who has lost everything is offered a second chance.\n",
      "Book description: 'I'll Take It Any Way You Want It (Silhouette Special Edition, No. 1473)' by Annette Broadrick.\n",
      "Tyndale's New Testament by William Tyndale (ISBN: 0300044194), Predicted Rating: 0.19, Description: This is a new translation of the 1526 edition of the New Testament in the English language. This edition is printed on the same paper as the original 1526 edition. The 1526 edition is also known as 'Tyndale's New Testament'. The 1526 edition is also known as 'Tyndale's New Testament'. The 1526 edition is also known as 'Tyndale's New Testament'. The 1526 edition is also known as 'Tyndale's New Testament\n",
      "Joe Pepper (Tales of Texas) by Elmer Kelton (ISBN: 0812561570), Predicted Rating: 0.22, Description: Joe Pepper is a cowboy, a rancher and a sheriff. He's a good man, a hard worker and a tough man. He's also a little bit of a coward. But he's got a good heart and he's got a good head on his shoulders. He's a good man. And he's got a good story.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def recommend_books_dl(user_id, n=5, filter_rated=True, batch_size=256):\n",
    "    \"\"\"Recommends books for a user using the trained NCF model with book descriptions and scaled ratings.\"\"\"\n",
    "\n",
    "    if user_id not in user_map:\n",
    "        print(f\"User {user_id} not found. Returning popular books instead.\")\n",
    "        return recommend_popular_books(n)\n",
    "\n",
    "    model.eval()  \n",
    "\n",
    "    user_idx = torch.tensor([user_map[user_id]] * num_books, dtype=torch.long).to(device)\n",
    "    book_indices = torch.arange(num_books, dtype=torch.long)\n",
    "\n",
    "    predicted_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_books, batch_size):  # Process books in batches\n",
    "            batch_books = book_indices[i:i+batch_size].to(device)\n",
    "            batch_book_descs = book_desc_features[i:i+batch_size].to(device)  # Move only a batch\n",
    "\n",
    "            batch_scores = model(user_idx[i:i+batch_size], batch_books, batch_book_descs)\n",
    "            predicted_scores.append(batch_scores.cpu())  # Move results back to CPU\n",
    "\n",
    "    predicted_scores = torch.cat(predicted_scores).numpy().flatten()  # Convert to NumPy\n",
    "\n",
    "    #print(predicted_scores.dtype)\n",
    "    #print(predicted_scores[0:5])\n",
    "\n",
    "    # use valid indices \n",
    "    top_books_idx = np.argsort(predicted_scores)[::-1][:n]  \n",
    "    #top_books_idx = [idx for idx in top_books_idx if idx < num_books]  # Ensure within bounds\n",
    "\n",
    "    # all ISBNs exist in `book_map`\n",
    "    recommended_books_isbn = [list(book_map.keys())[i] for i in top_books_idx if i < len(book_map)]\n",
    "    #print(recommended_books_isbn)\n",
    "\n",
    "    # Avoid indexing errors when retrieving `Predicted-Rating`\n",
    "    recommended_books_df = books_filtered [books_filtered [\"ISBN\"].isin(recommended_books_isbn)].copy()\n",
    "    #recommended_books_df = books[books[\"ISBN\"].isin(recommended_books_isbn)].copy()\n",
    "\n",
    "    # Fix indexing to prevent errors\n",
    "    recommended_books_df[\"Predicted-Rating\"] = recommended_books_df[\"ISBN\"].map(lambda isbn: predicted_scores[book_map[isbn]] if isbn in book_map else np.nan)\n",
    "\n",
    "    if filter_rated:\n",
    "        rated_books = ratings[ratings[\"User-ID\"] == user_id][\"ISBN\"].tolist()\n",
    "        recommended_books_df = recommended_books_df[~recommended_books_df[\"ISBN\"].isin(rated_books)]\n",
    "\n",
    "    return recommended_books_df[[\"Description\", \"ISBN\", \"Book-Title\", \"Book-Author\", \"Predicted-Rating\"]].head(n).to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "recommendations_dl = recommend_books_dl(276729, 5)\n",
    "for book in recommendations_dl:\n",
    "    print(f\"{book['Book-Title']} by {book['Book-Author']} (ISBN: {book['ISBN']}), Predicted Rating: {book['Predicted-Rating']:.2f}, Description: {book['Description']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
